{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis\n",
    "\n",
    "To analyze our data, we will be looking at the csv files containing the tweets from each state before and after the election and performing sentiment analysis on them. We will go about this process by:\n",
    "1. Scraping the tweet text from the csv files\n",
    "2. Cleaning tweets to remove stopwords and other unneccesary parts\n",
    "3. Training a Naive Bayes classifier to classify tweets as positive or negative\n",
    "4. Running the classifier on our cleaned tweets to determine average sentiment\n",
    "We scrape the tweets from the csv file using scrape_tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scrape tweets from csv file\n",
    "def scrape_tweets(filename):\n",
    "    tweets = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) #skip header\n",
    "        data = [r for r in reader]\n",
    "        for item in data:\n",
    "            tweets.append(item[3])\n",
    "    return tweets   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to clean our scraped tweets. We do this using clean_tweets where we remove stopwords and other non-essential parts of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean scraped tweets\n",
    "def clean_tweets(tweets):\n",
    "    cleaned_tweets = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for tweet in tweets:\n",
    "        cleaned_tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet).lower()\n",
    "        for w in tweet.split(\" \"):\n",
    "            if w not in stop_words:\n",
    "                cleaned_tweet += w + \" \"\n",
    "        cleaned_tweet = cleaned_tweet.strip()\n",
    "        cleaned_tweets += [cleaned_tweet]\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train a naive bayes classifier to tag a tweet as positive or negative. We use NLTK's NaiveBayesClassifier to do so in train_naive_bayes using training data from pos_tweet.txt and neg_tweets.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train naive bayes classifier\n",
    "#some of this code has been adapted from https://www.twilio.com/blog/2017/09/sentiment-analysis-python-messy-data-nltk.html\n",
    "def format_sentence(s):\n",
    "    return({word: True for word in nltk.word_tokenize(s)})\n",
    "\n",
    "def train_naive_bayes():\n",
    "    positive = []\n",
    "    with open(\"./pos_tweets.txt\") as f:\n",
    "        for i in f: \n",
    "            positive.append([format_sentence(i), 'positive'])\n",
    "    negative = []\n",
    "    with open(\"./neg_tweets.txt\") as f:\n",
    "        for i in f: \n",
    "            negative.append([format_sentence(i), 'negative'])  \n",
    "    training = positive + negative\n",
    "    classifier = NaiveBayesClassifier.train(training)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a classifer from the previous step, we can evaluate our cleaned tweets. We determine average sentiment using evaluate_sentiment where we calculate the ratio of positive tweets to total tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#determine average sentiment accross tweets\n",
    "def evaluate_sentiment(tweets, classifier):\n",
    "    total_tweets = len(tweets)\n",
    "    positive_tweets = 0\n",
    "    for tweet in tweets:\n",
    "        sentiment = classifier.classify(format_sentence(tweet))\n",
    "        if sentiment == \"positive\":\n",
    "            positive_tweets += 1\n",
    "    average_happiness = positive_tweets/total_tweets\n",
    "    return average_happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have all of the individual steps hammered down now, we can put them together to perform sentiment analysis on our 100 csv files. We do this using perform_sentiment_analysis. This returns a dictionary with the average sentiment of each state before and after the election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#determine average sentiment accross all states\n",
    "def perform_sentiment_analysis(csv_files):\n",
    "    results = dict()\n",
    "    classifier = train_naive_bayes() #only need to train once\n",
    "    for file in csv_files:\n",
    "        tweets = scrape_tweets(file)\n",
    "        cleaned_tweets = clean_tweets(tweets)\n",
    "        average_happiness = evaluate_sentiment(cleaned_tweets, classifier)\n",
    "        key = file.replace(\".csv\", \"\")\n",
    "        results[key] = average_happiness\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ak_after.csv', 'ak_before.csv', 'al_after.csv', 'al_before.csv', 'ar_after.csv', 'ar_before.csv', 'az_after.csv', 'az_before.csv', 'ca_after.csv', 'ca_before.csv', 'co_after.csv', 'co_before.csv', 'ct_after.csv', 'ct_before.csv', 'de_after.csv', 'de_before.csv', 'fl_after.csv', 'fl_before.csv', 'ga_after.csv', 'ga_before.csv', 'hi_after.csv', 'hi_before.csv', 'ia_after.csv', 'ia_before.csv', 'id_after.csv', 'id_before.csv', 'il_after.csv', 'il_before.csv', 'in_after.csv', 'in_before.csv', 'ks_after.csv', 'ks_before.csv', 'ky_after.csv', 'ky_before.csv', 'la_after.csv', 'la_before.csv', 'ma_after.csv', 'ma_before.csv', 'md_after.csv', 'md_before.csv', 'me_after.csv', 'me_before.csv', 'mi_after.csv', 'mi_before.csv', 'mn_after.csv', 'mn_before.csv', 'mo_after.csv', 'mo_before.csv', 'ms_after.csv', 'ms_before.csv', 'mt_after.csv', 'mt_before.csv', 'nc_after.csv', 'nc_before.csv', 'nd_after.csv', 'nd_before.csv', 'ne_after.csv', 'ne_before.csv', 'nh_after.csv', 'nh_before.csv', 'nj_after.csv', 'nj_before.csv', 'nm_after.csv', 'nm_before.csv', 'nv_after.csv', 'nv_before.csv', 'ny_after.csv', 'ny_before.csv', 'oh_after.csv', 'oh_before.csv', 'ok_after.csv', 'ok_before.csv', 'or_after.csv', 'or_before.csv', 'pa_after.csv', 'pa_before.csv', 'ri_after.csv', 'ri_before.csv', 'sc_after.csv', 'sc_before.csv', 'sd_after.csv', 'sd_before.csv', 'tn_after.csv', 'tn_before.csv', 'tx_after.csv', 'tx_before.csv', 'ut_after.csv', 'ut_before.csv', 'va_after.csv', 'va_before.csv', 'vt_after.csv', 'vt_before.csv', 'wa_after.csv', 'wa_before.csv', 'wi_after.csv', 'wi_before.csv', 'wv_after.csv', 'wv_before.csv', 'wy_after.csv', 'wy_before.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = []\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ak_after.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2ac74207030c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperform_sentiment_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-fdabb8f76df1>\u001b[0m in \u001b[0;36mperform_sentiment_analysis\u001b[0;34m(csv_files)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_naive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#only need to train once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcleaned_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maverage_happiness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0805627186b8>\u001b[0m in \u001b[0;36mscrape_tweets\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#skip header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ak_after.csv'"
     ]
    }
   ],
   "source": [
    "print(perform_sentiment_analysis(csv_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
